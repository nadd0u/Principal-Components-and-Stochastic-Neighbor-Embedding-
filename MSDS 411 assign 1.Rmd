---
title: 'Principal Components (PCA) and Stochastic Neighbor Embedding
  (t-SNE)'
author: "Nadia Noui-Mehidi"
output:
  github_document: default
  rmarkdown::github_documen: default
  
---

```{r setup, include=FALSE}

#"/Users/nadianoui-mehidi/Cluster-Analysis"
dir.create("raw_data")
file.copy("stockdata.csv", "raw_data")
library(readr)
my.data <- read.csv("raw_data/stockdata.csv")


```
Our data consists of daily closing stock prices for twenty stocks and a large-cap index fund from Vanguard (VV). A summary and the first 6 rows of the data set are below.

```{r}
str(my.data)
head(my.data)
```
Our data has 21 numerical variables; our target variable "VV" representing our market index and 10 explanatory variables representing a stock price. There is also a date variable representing the calendar date. Each record represents the closing stock prices and market index for a particular day. We want to use the individual stock amounts to explain the variation in the market index. We will explore this concept using both linear regression and principal components analysis.

## 1. Standardization of Variables

Let’s begin our analysis with some data preparation. It is usually beneficial for each variable to be centered at zero for PCA, due to the fact that it makes comparing each principal component to the mean straightforward. This also eliminates potential problems with the scale of each variable. We standardize each of our numeric variables with a log return transformation and store the standardized variables in a new dataset. The first 6 observations in our transformed  dataset is below: 

```{r}
# Note Date is a string of dd-Mon-yy in R this is '%d-%B-%y';
my.data$RDate <- as.Date(my.data$Date,'%d-%B-%y');
sorted.df <- my.data[order(my.data$RDate),];


AA <- log(sorted.df$AA[-1]/sorted.df$AA[-dim(sorted.df)[1]]);

# Manually check the first entry: log(9.45/9.23)
# Type cast the array as a data frame;
returns.df <- as.data.frame(AA);
str(returns.df)
returns.df$BAC <- log(sorted.df$BAC[-1]/sorted.df$BAC[-dim(sorted.df)[1]]);
returns.df$BHI <- log(sorted.df$BHI[-1]/sorted.df$BHI[-dim(sorted.df)[1]]);
returns.df$CVX <- log(sorted.df$CVX[-1]/sorted.df$CVX[-dim(sorted.df)[1]]);
returns.df$DD  <- log(sorted.df$DD[-1]/sorted.df$DD[-dim(sorted.df)[1]]);
returns.df$DOW <- log(sorted.df$DOW[-1]/sorted.df$DOW[-dim(sorted.df)[1]]);
returns.df$DPS <- log(sorted.df$DPS[-1]/sorted.df$DPS[-dim(sorted.df)[1]]);
returns.df$GS  <- log(sorted.df$GS[-1]/sorted.df$GS[-dim(sorted.df)[1]]);
returns.df$HAL <- log(sorted.df$HAL[-1]/sorted.df$HAL[-dim(sorted.df)[1]]);
returns.df$HES <- log(sorted.df$HES[-1]/sorted.df$HES[-dim(sorted.df)[1]]);
returns.df$HON <- log(sorted.df$HON[-1]/sorted.df$HON[-dim(sorted.df)[1]]);
returns.df$HUN <- log(sorted.df$HUN[-1]/sorted.df$HUN[-dim(sorted.df)[1]]);
returns.df$JPM <- log(sorted.df$JPM[-1]/sorted.df$JPM[-dim(sorted.df)[1]]);
returns.df$KO  <- log(sorted.df$KO[-1]/sorted.df$KO[-dim(sorted.df)[1]]);
returns.df$MMM <- log(sorted.df$MMM[-1]/sorted.df$MMM[-dim(sorted.df)[1]]);
returns.df$MPC <- log(sorted.df$MPC[-1]/sorted.df$MPC[-dim(sorted.df)[1]]);
returns.df$PEP <- log(sorted.df$PEP[-1]/sorted.df$PEP[-dim(sorted.df)[1]]);
returns.df$SLB <- log(sorted.df$SLB[-1]/sorted.df$SLB[-dim(sorted.df)[1]]);
returns.df$WFC <- log(sorted.df$WFC[-1]/sorted.df$WFC[-dim(sorted.df)[1]]);
returns.df$XOM <- log(sorted.df$XOM[-1]/sorted.df$XOM[-dim(sorted.df)[1]]);
returns.df$VV  <- log(sorted.df$VV[-1]/sorted.df$VV[-dim(sorted.df)[1]]);
str(returns.df)

head(returns.df)
```

## 2. Correlations between Explanatory Variables and our Target

We begin our exploratory data analysis by looking at our explanatory variable's correlation to our target variable "VV". 

```{r pressure, echo=FALSE}
# Compute correlation matrix for returns;
returns.cor <- cor(returns.df)
returns.cor[,c('VV')]

# Barplot the last column to visualize magnitude of correlations;
barplot(returns.cor[1:20,c('VV')],las=2,ylim=c(0,1.0))
title('Correlations with VV')
```
All our variables have a correlation to VV of at least 0.44. The variables most highly correlated to VV are MMM (0.76), WFC (0.73) XOM & CVX (0.72). This tells us our variables are not independent from the target which means there is some underlying pattern to uncover. Since all of our variables are strongly correlated to the target we may have some multicollinearity in our data. 

## 3. Pairwise Correlations 

Multicollinearity exists among the explanatory variables when these variables are correlated among themselves. We use a corrplot to visualize the pairwise correlations between our explanatory variables. If correlation among pairs of variables are large this means we have redundancy in our data. This can have effects on the extra sums of squares, fitted values and predictions, regression coefficients, and many other parts of multiple linear regression.


```{r}
# Make correlation plot for returns;
require(corrplot)
corrplot(returns.cor)
```
Our corrplot seems to indicate heavy correlation between most of the explanatory variables.
Variables that look like they have expecially large covariance are XOM WFC SLB HON HAL GS MMM CVX. These variables look like they are theyre statistically dependent or in other words there is a lot of redundancy between the variables. They probably have high VIF levels.
Only three variables (DPS, MPC, PEP) look like they have relatively low covariance. This means theyre statistically independent or orthogonal, they dont move together. They probably have low VIF values. 

For a more rigorous examination of multicollinearity we should regress these variables.

## 4 Linear Regression
A linear regression model can be fitted and collinearity between predictors detected using Variance Inflation Factor (VIF). 

With VV as our target variable we create two linear regressions. A full model using all the explanatory variables and simple model including only the following explanatory variables: 
GS+DD+DOW+HON+HUN+JPM+KO+MMM+XOM


```{r}
require(car)
# Fit some model
model.1 <- lm(VV ~ GS+DD+DOW+HON+HUN+JPM+KO+MMM+XOM, data=returns.df)
summary(model.1)


# Fit the full model
model.2 <- lm(VV ~ BAC+GS+JPM+WFC+BHI+CVX+DD+DOW+DPS+HAL+HES+HON+HUN+KO+MMM+MPC+PEP+SLB+XOM,data=returns.df)
summary(model.2)

```
After running our regressions we see if the analysis exhibits any signs of multicollinearity. Some indicators are that the F-test for overall model is significant but the t-tests for individual coefficient estimates are not, the R-square value is large but none of the beta weights is statistically significant, or coefficient estimates vary from model to model

We evaluate our models' fit with the ANOVA test. The F statistic is highly significant for both models, so the model fits the data better than guessing the mean. Our R squared values of 84 and 87%  tell us the percentage from the dependant variable’s variation which is explained by the model.

We look at each predictors coefficient and its significance level to determine which explanatory variables contribute to the model. In our partial model, all our variables are highly significant but in the full model seven of our variables, including ones that were included in the partial model, are no longer significantly different from 0. This suggests we do have a problem with multicollinearity. By overinflating the standard errors, multicollinearity makes some variables statistically insignificant when they should be significant. 

Multicollinearity is also measured using a Variance Inflation Factor (VIF). VIF is a measure of how much the variance of the estimated regression coefficient, can be inflated by the existence of correlations among the predictor variables in the model. If the VIF is equal to 1 there is no multicollinearity among factors, but if the VIF is greater than 1, the predictors may be moderately correlated.
The VIF for the partial and full models are below 

```{r}
print("Partial Model VIF:")
vif(model.1)
print("Full Model VIF:")
vif(model.2)
```
The output above shows that every variable has at least moderate VIF (1.5 or higher). Most variables have a VIF greater than 2 and the highest VIF is GS at 3.2. A VIF between 5 and 10 is usually considered problematic, being a clue for predictors multicollinearity. Our variables indicate moderate multicollinearity but not enough to assume that the regression coefficients are poorly estimated due to multicollinearity.

## 5. Principal Component Analysis

Principal Component Analysis (PCA) is a method of dimension reduction that is particularly useful when the variables within the data set are highly correlated. Correlation indicates a redundancy in the data. PCA can be used to reduce the original variables into a smaller number of new variables (components) that explain most of the variance in the original variables. 

### Choosing Components 

The first principal component of a data set is the linear combination of the explanatory variables that explains the largest amount of variance in the data. The second principal component is the linear combination of variables that has maximal variance out of all linear combinations that are uncorrelated with - or orthigonal to- our first component. Each subsequent component explains less of the variation than the previous one. The components we select become the axis of a new low dimensional coordinate system; the goal when selecting components is to reduce dimenionality while preserving the underlying relationship between variables. 

A summary of our principal components is below: 

```{r}
returns.pca <- princomp(x=returns.df[,-21],cor=TRUE)
summary(returns.pca)
plot(returns.pca)
```
From the output we can see that 48.2% of the variation in the data can be explained by the first component alone, and 56% is explained by the first two components. In using PCA for dimension reduction, after we compute the principal components we need to decide how many principal components to keep or retain.
We could use a 'total variance explained' decision rule to choose the number of components to use. For example, we could say that we want an explained variance of at least 80%. In this case, to get 80% of variance explained we would choose the first 8 principal components. 
Another tool for choosing the number of principal components is to make a scree plot. 
A scree plot shows the proportion of variance explained by each Principal Component. There is often a break point between a steep part of the line and a shallower part of the line (the elbow). We could use this elbow as our break off point. In this case, since the proportion of variance explained for our data drops off significantly after the first component and remains flat beyond that, the scree plot isnt that helpful. We will use the 80% thresehold as our decision rule and keep the first 8 components. 


```{r}
# Make Scree Plot
scree.values <- (returns.pca$sdev^2)/sum(returns.pca$sdev^2);

plot(scree.values,xlab='Number of Components',ylab='',type='l',lwd=2)
points(scree.values,lwd=2,cex=1.5)
title('Scree Plot')


# Make Proportion of Variance Explained
variance.values <- cumsum(returns.pca$sdev^2)/sum(returns.pca$sdev^2);

plot(variance.values,xlab='Number of Components',ylab='',type='l',lwd=2)
points(variance.values,lwd=2,cex=1.5)
abline(h=0.8,lwd=1.5,col='red')
abline(v=8,lwd=1.5,col='red')
text(13,0.5,'Keep 8 Principal Components',col='red')
title('Total Variance Explained Plot')
```

###Loadings
The coefficients of the linear combination that make up our Principal Component can be described by a vector called the loading. The loading can tell us something about the underylying phenomenon causing the variation, for example it could be comprimised by variables that together describe some latent variable that we can name. The individual coefficients tells us the correlation between the original variable and the axis; they indicate the direction and magnitude in the increase in that variable. A larger coefficient indicates that the variable is important in explaining the difference between the samples among that axis. The loading vector can help us decide which variables are important and which (if any) can be dropped from the data set.

The principal components for our data and their associated loading vectors are below. 

```{r}

returns.pca$loadings 
#returns.pca$scores 

```


Our first principal component's loading vector includes all of our explanatory variables and each coefficient expolaines a relatively equal amount of the data variation. For this reason, we are not going to eliminate any of our variables for analysis. 


### Plotting Obvervations on a Low Dimensional Plot
We want to plot our data points on a two dimesional coordinate system whose axis are first two principal components. Visualizing all observations in a 2-d coordinate system can help us see underlying relationships between data points. 
We are using a biplot to represent both the observations and variables of multivariate data on the same plot.
The biplot is made up of:
The score of each observation on the first two principal components
The loading of each variable on the first two principal components.

The axes at the bottom and left of the biplot are the coordinate axes for the observations. The axes at the top and right of the biplot are the coordinate axes for the vectors.

In our biplot below, the two principal components account for approximately 56% of the variance in the data. This means that the biplot is an ok but not good approximation to the data.

```{r}
pc.1 <- returns.pca$loadings[,1];
pc.2 <- returns.pca$loadings[,2];

library(ggbiplot)
#ggbiplot(returns.pca, labels=rownames(pc.1))

library(ggfortify)
pca.plot <- autoplot(returns.pca, shape = FALSE, label.size = 1, loadings = TRUE, loadings.colour = 'orange', loadings.label = TRUE, loadings.label.size = 2, scale=0 )
pca.plot

```
The cosine of the angle between a vector and an axis indicates the importance of the contribution of the corresponding variable to the axis dimension. all the arrows contribute about the same amount to PCA1, though DPS contributes a little less than everyone else. DPS PEP KO contribute the most to PCA2 and DD contributes the least 

The cosine of the angle between vectors indicates correlation between variables. Highly correlated variables (ie. DPS, KO, PEP) point in the same direction; uncorrelated variables are at right angles to each other (ie. DPS & BAC). 

Points that are close to each other in the biplot represent observations that are close together in space. If the data includes a categorical variables, we could color code the points by category to see whether observations cluster. For example, if we thought time of year effected stock price, we could have created a categorical variable for Season or Month or Fiscal Quarter and color coded the scores accordingly.

Note: This assignment asked us to color code our observations by Industry. Industry has a relationship to Stocks which are the explanatory variables (columns) instead of the observations which are made up of many stocks and therefore many industries. Since the relationship between Stock Industry and Observation is not one-to-one I didnt know how to proceed with this part of the assignement. I had the same problem color coding my tSNE graph.  

##7. Principal Component Regression
Now let’s use principal components in predictive modeling.  The predictor variables for our predictive model will be the PCA scores. 
Build a PCA model on the data in X, fitting the 8 components we selected. 
n PCR, instead of regressing the dependent variable on the explanatory variables directly, the principal components of the explanatory variables are used as regressors. 

(What are the scores?)  In order for us to be building predictive models, we need to have a train data set and a test data set so let’s split our data into train and test data sets manually by generating a uniform(0,1) random variable and attaching it to the data frame.  


```{r}
# Create the data frame of PCA predictor variables;
return.scores <- as.data.frame(returns.pca$scores);
return.scores$VV <- returns.df$VV;
return.scores$u <- runif(n=dim(return.scores)[1],min=0,max=1);
#head(return.scores)

# Split the data set into train and test data sets;
train.scores <- subset(return.scores,u<0.70);
test.scores <- subset(return.scores,u>=0.70);
#dim(train.scores)
#dim(test.scores)
#dim(train.scores)+dim(test.scores)
#dim(return.scores)

# Fit a linear regression model using the first 8 principal components;
pca1.lm <- lm(VV ~ Comp.1+Comp.2+Comp.3+Comp.4+Comp.5+Comp.6+Comp.7+Comp.8, data=train.scores);
summary(pca1.lm)

# Compute the Mean Absolute Error on the training sample;
pca1.mae.train <- mean(abs(train.scores$VV-pca1.lm$fitted.values));
#vif(pca1.lm)

# Score the model out-of-sample and compute MAE;
pca1.test <- predict(pca1.lm,newdata=test.scores);
pca1.mae.test <- mean(abs(test.scores$VV-pca1.test));
```



Using our train and test data sets, we fit a linear regression model using the first eight principal components and compute the Mean Absolute Error (MAE) for that model.  We will call this model pca1.lm.  We score our model on our test data set and compute the out-of-sample MAE.
Note: The VIF values associated with every predictor variable in any principal components regression model should all be one.  Why?
  
##8. Comparing Models (Principal Component Regression Vs Linear Regressions) 
We will use MAE to compare the accuracy of our predictive models. MAE tells us the average of the errors between our modles predictions and the actual value. After running our PCA and linear regression models through the test and training datasets we come up with the MAE table that you see below:

```{r}

# Let's compare the PCA regression model with a 'raw' regression model;
# Create a train/test split of the returns data set to match the scores data set;
returns.df$u <- return.scores$u;
train.returns <- subset(returns.df,u<0.70);
test.returns <- subset(returns.df,u>=0.70);
#dim(train.returns)
#dim(test.returns)
#dim(train.returns)+dim(test.returns)
#dim(returns.df)


# Fit model.1 on train data set and 'test' on test data;
model.1 <- lm(VV ~ GS+DD+DOW+HON+HUN+JPM+KO+MMM+XOM, data=train.returns)
model1.mae.train <- mean(abs(train.returns$VV-model.1$fitted.values));
model1.test <- predict(model.1,newdata=test.returns);
model1.mae.test <- mean(abs(test.returns$VV-model1.test));


# Fit model.2 on train data set and 'test' on test data;
model.2 <- lm(VV ~ BAC+GS+JPM+WFC+BHI+CVX+DD+DOW+DPS+HAL+HES+HON+HUN+KO+MMM+MPC+PEP+SLB+XOM, data=train.returns)
model2.mae.train <- mean(abs(train.returns$VV-model.2$fitted.values));
model2.test <- predict(model.2,newdata=test.returns);
model2.mae.test <- mean(abs(test.returns$VV-model2.test));
#create a table of the MAE values from pca1.lm model.1 model.2

mae <- matrix(c( 0.002221394,0.002110697,0.00199422,0.001924341, 0.002068513, 0.001939296),ncol=2,byrow=TRUE)
colnames(mae) <- c("train","test")
rownames(mae) <- c("model1","model2","pca1")
mae <- as.table(mae)
mae
```

Our full linear model (model2) has the smallest average error on both the training and the test data sets. This may be because we didnt use enough PCs to capture enough of of the variation or we chose the wrong components. Since we chose our components in order to minimize multicollinearity and not for its predictive power we may just want to adjust our selection criterion to better align with our goal. 

## 9. PCA Regression using Backwards Selection
 
Our previous PCA regression model regressed the target variable against components chosen using a decision rule from the standard unsupervised learning problem. We chose to retain the components which were important in the sense that they are relatively successful in explaining the total variability of the group of explanatory variables. 

This time, we use a automated supervised learning method called backwards selection to select our components based on their relatively importance as predictors of our target variable, i.e. the components with the largest correlation to the target.
 
```{r}
# remove u

train.scores <- train.scores[c(-22)]

# Fit full.lm on PCA scores of train data
full.lm <- lm(VV ~ ., data=train.scores);
summary(full.lm)


library(MASS)
backward.lm <- stepAIC(full.lm,direction=c('backward'))
summary(backward.lm)
backward.mae.train <- mean(abs(train.scores$VV-backward.lm$fitted.values));
vif(backward.lm)

backward.test <- predict(backward.lm,newdata=test.scores);
backward.mae.test <- mean(abs(test.scores$VV-backward.test))
```

The backwards selection method starts with all components and tests each one, deleting the component whose loss gives the most statistically insignificant deterioration of model fit. This process is repeated until no further components can be deleted without a statistically significant loss of fit. Using this component selection criterion, our supervising learning method selects the following 10 principal components: 
Comp.1, Comp.2, Comp.3, Comp.5, Comp.8, Comp.9, Comp.10, Comp.11, Comp.12. Comp.14.


We compare our backward.lm model to our previous models in the MAE table below. This time, our backwards.lm model performed better than the other three on both the training and testing data sets. The appropriate criterion vwhen selecting componenets for our predictive model was the one that maximixes the models predictive power.
```{r}
#model1.mae.train
#model1.mae.test
#model2.mae.train
#model2.mae.test
#pca1.mae.train
#pca1.mae.test
#backward.mae.train
#backward.mae.test
mae <- matrix(c( 0.002221394,0.002110697,0.00199422,0.001924341, 0.002068513, 0.001939296, 0.002008742, 0.00188875),ncol=2,byrow=TRUE)
colnames(mae) <- c("train","test")
rownames(mae) <- c("model1","model2","pca1","backward")
mae <- as.table(mae)
mae

```


##10. T-SNE
T-sne models position high-dimensional object in low dimensional space in such a way that similar points are positioned close to one another in space and dissimilar points are positioned far from one another. PCA focuses on placing dissimilar data points far apart in a lower dimension representation so unlike PCA, t-SNE preserves locality.


We conduct a t-SNE analysis on the Stock Portfolio data and plot our observations in a 2 dimensional plot. 
```{r}
library(Rtsne)
library(tidyverse)
set.seed(1) # for reproducibility
tsne <- Rtsne(returns.df, dims = 2, perplexity=30, verbose=TRUE, max_iter = 5000, learning = 200)
```
```{r}
plot(tsne$Y, t='n', main="tSNE")
text(tsne$Y, labels=rownames(returns.df))
```


This is already a significant improvement over the PCA visualization we created earlier. You can see that the digits are very clearly clustered in their own little group. Unlike our PCA plot, our t-SNE plot shows a distinct pattern in the data. It looks like there are 13ish clusters that form a concave kind of U shape in the dimension space. 



#(11) Reflections 

When it came to capturing the underlying pattern in the data, t-SNE did a better job than PCA. The visualization created using t-SNE shows both a general pattern in the data as well as local clusters. It revealed a non-linear pattern in the data which is probably why it worked better  than the PCA which is a linear algorithm and cant interpret a polynomial relationship. If I were able to color code the data we may have seen some grouping in the PCA visualization, but it wouldnt have been as well defined as the t-SNE graph.  When it came to modeling, PCA improved our linear regression when we selected componenets using a supervised learning method. 
 
